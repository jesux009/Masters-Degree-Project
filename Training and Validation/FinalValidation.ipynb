{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8009157,"sourceType":"datasetVersion","datasetId":4717401},{"sourceId":8323719,"sourceType":"datasetVersion","datasetId":4944459},{"sourceId":8326623,"sourceType":"datasetVersion","datasetId":4945357},{"sourceId":8333462,"sourceType":"datasetVersion","datasetId":4948716}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms.functional as T\nimport numpy as np\n\n# Generalized UNet architecture from scratch for learning purposes. See https://arxiv.org/abs/1505.04597\n\nclass CNNBlock(nn.Module):\n\n     # In the UNet architecture, this is the smallest possible block.\n     # Each of these applies a 2D convolution, normalizes the output and passes it through a ReLU activation function.\n     # The result of passing an input of size [N, in, H_in, W_in] through this layer is an output [N, out, H_out, W_out]\n     #    where H_out = (H_in + 2*padding - dilation*(kernel_size-1) - 1)/stride + 1 \n     #          W_out = (W_in + 2*padding - dilation*(kernel_size-1) - 1)/stride + 1 \n     # This output corresponds to the element_wise result of the ReLU activation function of the normalized convolution output.\n\n     # The block is used several times at each \"level\", which is why we later define a CNNSet, which is composed of several \n     # usages of the CNNBlock sequentially \n\n     \"\"\"\n     Parameters:\n     in_channels (int): Number of channels in the input to the block.\n     out_channels (int): Number of channels produced by the convolution of the input within the block.\n     kernel_size (int): Size of the convolving kernel. Default = 3.\n     stride (int) : Stride of the convolution. Default = 1.\n     padding (int) : Padding added to all four sides of the input. Default = 0.\n     \"\"\"\n\n     def __init__(self, in_channels: int, out_channels:int, kernel_size:int=3, stride:int=1, padding:int=0):\n          super(CNNBlock, self).__init__()\n\n          self.sequential_block = nn.Sequential(\n               nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n               nn.BatchNorm2d(out_channels),\n               nn.ReLU(inplace=True)\n          )\n\n     def forward(self, x):\n          x = self.sequential_block(x)\n          return x\n     \n\n\nclass Encoder(nn.Module):\n\n     # The encoder part of the UNet consists of the downsampling of the initial input through the application of \n     # a sequential series of convolution sets followed by a max pooling layer. The complete operation's objective \n     # is to capture the context and spatial information of the input image at different scales.\n\n     \"\"\"\n     Parameters:\n     in_channels (int): Number of input channels of the first CNNSet.\n     out_channels (int): Number of output channels of the first CNNSet.\n     padding (int): Padding applied in each convolution.\n     levels (int): Number times a CNNSet + MaxPool2D layer is applied.\n     \"\"\"\n\n     def __init__(self, input_channels:int, output_channels:int, pool_kernelsize: int, parameters: list):\n          super(Encoder, self).__init__()\n          self.encoder_layers = nn.ModuleList()\n          levels = len(parameters)\n          for level in range(levels-1):\n               for conv in range(len(parameters[level])):\n                    conv_kernelsize = parameters[level][conv][0]\n                    conv_stride = parameters[level][conv][1]\n                    self.encoder_layers.append(CNNBlock(in_channels=input_channels, out_channels=output_channels, kernel_size=conv_kernelsize, stride=conv_stride))\n                    input_channels = output_channels\n               output_channels *= 2\n               self.encoder_layers.append(nn.MaxPool2d(pool_kernelsize))\n          # A final convolution set is applied after all the levels, commonly referred to as the bottleneck\n          for conv in range(len(parameters[-1])):\n               conv_kernelsize = parameters[-1][conv][0]\n               conv_stride = parameters[-1][conv][1]\n               self.encoder_layers.append(CNNBlock(in_channels=input_channels, out_channels=output_channels, kernel_size=conv_kernelsize, stride=conv_stride))\n               input_channels = output_channels\n\n     def forward(self,x):\n          residual_connection = []\n          for i, layer in enumerate(self.encoder_layers):\n               x = layer(x)\n               # After the set CNN is processed, the result is logged to be sent in a connection to the decoder\n               if i<len(self.encoder_layers)-1 and isinstance(self.encoder_layers[i+1], nn.MaxPool2d):\n                    residual_connection.append(x)\n               # If the processed layer is a pooling operation, the result is not logged\n          return x, residual_connection\n\n\n\nclass Decoder(nn.Module):\n\n     \"\"\"\n     Parameters:\n     in_channels (int): Number of input channels of the first up-convolution layer.\n     out_channels (int): Number of output channels of the first up-convolution layer.\n     padding (int): Padding applied in each convolution.\n     levels (int): number times an up-convolution + CNNSet is applied.\n     \"\"\"\n\n     # After the encoder has downsampled the information, the decoder now applies an upsampling to match to original features.\n     # This is achieved combining up-convolutions followed by convolution sets sequentially, achieving a recovery of the \n     # fine-grained spatial information lost during the downsampling in the encoder.\n\n     def __init__(self, input_channels:int, exit_channels:int, uppool_kernelsize:int, parameters: list):\n          super(Decoder, self).__init__()\n          self.exit_channels = exit_channels\n          self.decoder_layers = nn.ModuleList()\n\n          levels = len(parameters)\n          for level in range(levels-1):\n               output_channels = int(input_channels/2)\n               self.decoder_layers.append(nn.ConvTranspose2d(in_channels=input_channels, out_channels=output_channels, kernel_size=uppool_kernelsize, stride=uppool_kernelsize))\n               for conv in range(len(parameters[level])):\n                    conv_kernelsize = parameters[level][conv][0]\n                    conv_stride = parameters[level][conv][1]\n                    self.decoder_layers.append(CNNBlock(in_channels=input_channels, out_channels=output_channels, kernel_size=conv_kernelsize, stride=conv_stride))\n                    input_channels = output_channels\n          # A final convolution set without the ReLU activation function since the output will be passed through a BCELoss \n          self.decoder_layers.append(nn.Conv2d(in_channels=input_channels, out_channels=exit_channels, kernel_size=1))\n\n     def forward(self, x, residual_connection):\n          for i, layer in enumerate(self.decoder_layers):\n               # After the previous output is up-sampled, the connection from the equivalent level is concatenated\n               if i>0 and isinstance(self.decoder_layers[i-1], nn.ConvTranspose2d):\n                    # First we center-crop the route tensor to make the size match\n                    residual_connection[-1] = T.center_crop(residual_connection[-1], x.shape[2])\n                    # Then we concatenate the tensors in the dimensions of the channels\n                    x = torch.cat([x, residual_connection.pop(-1)], dim=1)\n                    x = layer(x)\n               # If the processed layer is an up-convolution operation, the connection is not performed\n               else:\n                    x = layer(x)\n          return x\n\n\nclass UNetV2(nn.Module):\n\n     \"\"\"\n     Parameters:\n     in_channels (int): Number of input channels.\n     first_out_channels (int): Number of output channels of the first convolution set.\n     exit_channels (int): Number of output channels.\n     levels (int): Number of levels for the encoder-decoder architecture.\n     padding (int): Padding applied in each convolution operation.\n     \"\"\"\n\n     # After the encoder has downsampled the information, the decoder now applies an upsampling to match to original features.\n     # This is achieved combining up-convolutions followed by convolution sets sequentially, achieving a recovery of the \n     # fine-grained spatial information lost during the downsampling in the encoder.\n\n     def __init__(self, in_channels, first_out_channels, exit_channels, pool_kernelsize, down_parameters, up_parameters, augment=False):\n          super(UNetV2, self).__init__()\n          levels = len(down_parameters)\n          self.encoder = Encoder(input_channels=in_channels, output_channels=first_out_channels, pool_kernelsize=pool_kernelsize, parameters=down_parameters)\n          self.decoder = Decoder(input_channels=first_out_channels*(2**(levels-1)), exit_channels=exit_channels, uppool_kernelsize=pool_kernelsize, parameters=up_parameters)\n          self.augment = augment\n        \n     def forward(self, x):\n          encoder_out, residuals = self.encoder(x)\n          decoder_out = self.decoder(encoder_out, residuals)\n          if self.augment: \n              return T.center_crop(decoder_out, (512,512))\n          else:\n              return T.center_crop(decoder_out, (256,256))","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:42:28.666837Z","iopub.execute_input":"2024-05-06T08:42:28.667142Z","iopub.status.idle":"2024-05-06T08:42:35.082028Z","shell.execute_reply.started":"2024-05-06T08:42:28.667117Z","shell.execute_reply":"2024-05-06T08:42:35.081240Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nimport os\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport albumentations as A\n\ndef create_grid(nc: int, offset=0.5) -> torch.Tensor:\n    grid = np.zeros((nc, nc, 2), dtype=np.float32)\n    for ix in range(nc):\n        for iy in range(nc):\n            grid[ix, iy, 1] = -1 + 2 * (ix + 0.5) / nc + offset / 128\n            grid[ix, iy, 0] = -1 + 2 * (iy + 0.5) / nc + offset / 128\n    grid = torch.from_numpy(grid).unsqueeze(0)\n    return grid\n\ndef img2tensor(img, dtype: np.dtype = np.float32):\n    img = np.transpose(img, (2, 0, 1))\n    tensor = torch.from_numpy(img.astype(dtype, copy=False))\n    return tensor\n\nclass ContrailsDataset(Dataset):\n\n     def __init__(self, path, use='train', soft_labels=False, only_positives=True, repeat=1, augment=False):\n          if use == 'train' or use=='metrics':\n               train = True\n          else:\n               train = False\n          self.path = os.path.join(path, \"train\" if train else \"validation\", \"images\")\n          if only_positives:\n               positives_path = '/kaggle/input/positivess'\n               positives_file = np.load(os.path.join(positives_path,\"positive_train.npy\" if train else \"positive_validation.npy\"))\n               positives_fnames = [filename.split(\"\\\\\")[3] for filename in positives_file]\n               if use == 'train' or use == 'cross-validate':\n                    self.filenames = [filename.split(\".\")[0] for filename in os.listdir(self.path) if filename.split(\".\")[0] in positives_fnames]\n               elif use == 'metrics':\n                    self.filenames = random.sample([filename.split(\".\")[0] for filename in os.listdir(self.path) if filename.split(\".\")[0] in positives_fnames], 500)\n          else:\n               if use == 'train' or use == 'cross-validate':\n                    self.filenames = [filename.split(\".\")[0] for filename in os.listdir(self.path)]\n               elif use == 'metrics':\n                    self.filenames = random.sample([filename.split(\".\")[0] for filename in os.listdir(self.path)],500)\n          self.train = train\n          self.nc = 3\n          self.repeat = repeat\n          self.soft_labels = soft_labels\n          self.augment = augment \n          if self.augment:\n               self.grid = create_grid(512, offset=0.5)\n\n     def __len__(self):\n          return self.repeat * len(self.filenames)\n     \n     def __getitem__(self, index):\n          index = index % len(self.filenames)\n          try:\n               image = np.array(Image.open(os.path.join(self.path, self.filenames[index] + '.png')))\n               if self.soft_labels:\n                    mask  = np.load(os.path.join(self.path.replace('images','soft_label'), self.filenames[index] + '.npy'))\n               else:\n                    mask  = np.load(os.path.join(self.path.replace('images','ground_truth'), self.filenames[index] + '.npy'))\n               image_tensor, mask_tensor = img2tensor(image/255), img2tensor(mask)   # Sizes 3x256x256 and 1x256x256  \n               if self.augment:\n                    transform = A.Compose([A.RandomRotate90(p=0.05),A.HorizontalFlip(p=0.05)])\n                    image_tensor = T.resize(image_tensor,512)\n                    mask_sym = F.grid_sample(mask_tensor.unsqueeze(0), self.grid, mode='bilinear', padding_mode='border', \n                                             align_corners=False).squeeze(0)\n                    image_np = image_tensor.permute(1, 2, 0).numpy()\n                    mask_np = mask_sym.permute(1, 2, 0).numpy()\n                    # Apply augmentation\n                    aug = transform(image=image_np, mask=mask_np)\n                    transformed_image = aug['image'].transpose(2, 0, 1)\n                    transformed_mask = aug['mask'].transpose(2, 0, 1)\n                    image_tensor = torch.from_numpy(transformed_image)\n                    mask_tensor = torch.from_numpy(transformed_mask)\n               return image_tensor, mask_tensor\n          except Exception as e:\n               print(f\"\\n Error loading file: {e} \\n\")\n               return None, None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-06T08:42:35.083764Z","iopub.execute_input":"2024-05-06T08:42:35.084161Z","iopub.status.idle":"2024-05-06T08:42:36.109612Z","shell.execute_reply.started":"2024-05-06T08:42:35.084136Z","shell.execute_reply":"2024-05-06T08:42:36.108817Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef validate(net, usage, device, pad, test_batch_size=50, threshold=0.5):    \n\n     testset = ContrailsDataset(path='/kaggle/input/opencontrails-png/SingleFrame_PNG', use=usage, augment=False, only_positives=False)\n     testloader = torch.utils.data.DataLoader(testset,batch_size=test_batch_size, shuffle=True, num_workers=0)\n\n     criterion = nn.BCEWithLogitsLoss()\n\n     # Positive pixels labelled as positive\n     TP = 0\n     # Negative pixels labelled as negative\n     TN = 0\n     # Positive pixels labelled as negative\n     FN = 0\n     # Negative pixels labelled as positive\n     FP = 0\n\n     running_loss = 0\n\n     with torch.no_grad():\n          for i, data in enumerate(testloader):\n               images, labels = data\n               images, labels = images.to(device), labels.to(device)\n               images = F.pad(images,(pad,pad,pad,pad), mode='reflect')\n               outputs = torch.sigmoid(net(images))\n               outputs = T.center_crop(outputs, (256,256))\n               outputs = outputs.view(-1,1,256,256)\n               binary_outputs = (outputs > threshold).float()\n\n               loss = criterion(outputs, labels)\n               running_loss += loss.item()\n\n               TP += torch.sum((binary_outputs == 1) & (labels == 1)).item()\n               TN += torch.sum((binary_outputs == 0) & (labels == 0)).item()\n               FN += torch.sum((binary_outputs == 0) & (labels == 1)).item()\n               FP += torch.sum((binary_outputs == 1) & (labels == 0)).item()\n\n               print(f'Processing batch {i+1}/{len(testloader)}', end='\\r')\n\n     # Pixel Accuracy\n     PA = TP/(TP+TN+FP+FN) if (TP+TN+FP+FN)>0 else '-'\n     # Jaccard Coefficient\n     IoU = TP/(TP+FP+FN) if (TP+FP+FN)>0 else '-'\n     # Precision\n     precision = TP/(TP+FP) if (TP+FP)>0 else '-'\n     # Recall\n     recall = TP/(TP+FN) if (TP+FN)>0 else '-'\n     # F1 Score\n     F1 = 2*(precision*recall/(precision+recall)) if ((TP+FP)>0 and (TP+FN)>0 and TP>0) else '-'\n     # Dice Coefficient\n     dice = 2*TP/(2*TP+FP+FN) if (TP+FP+FN)>0 else '-'\n\n     return running_loss/len(testset), PA, IoU, precision, recall, F1, dice","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:42:36.110701Z","iopub.execute_input":"2024-05-06T08:42:36.111083Z","iopub.status.idle":"2024-05-06T08:42:36.125678Z","shell.execute_reply.started":"2024-05-06T08:42:36.111039Z","shell.execute_reply":"2024-05-06T08:42:36.124835Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# !pip install segmentation-models-pytorch\n\n# import segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:42:36.127931Z","iopub.execute_input":"2024-05-06T08:42:36.128541Z","iopub.status.idle":"2024-05-06T08:42:36.139262Z","shell.execute_reply.started":"2024-05-06T08:42:36.128517Z","shell.execute_reply":"2024-05-06T08:42:36.138437Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def parameter_generator(levels, convs, kernel_size):\n     parameters = [[None for _ in range(convs)] for _ in range(levels)]\n     for level in range(levels):\n          for conv in range(convs):\n               if isinstance(kernel_size,list):\n                    parameters[level][conv] = [(kernel_size[level], kernel_size[level]), 1]\n               else:\n                    parameters[level][conv] = [(kernel_size, kernel_size), 1]\n     return parameters\n\n\nlevs = 5\ncons = 4\nkers = 3\npath2trained = '/kaggle/input/trained0705/UNETv2_5431_Positives_20epoch.pth'\npad = 188\n\nstate_dict = torch.load(path2trained)\ndownparameters = parameter_generator(levs,cons,kers)\nupparameters = parameter_generator(levs,cons,kers)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:42:36.140097Z","iopub.execute_input":"2024-05-06T08:42:36.140322Z","iopub.status.idle":"2024-05-06T08:42:40.035170Z","shell.execute_reply.started":"2024-05-06T08:42:36.140302Z","shell.execute_reply":"2024-05-06T08:42:40.034341Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"net = UNetV2(in_channels=3, first_out_channels=64, exit_channels=1, pool_kernelsize=2, down_parameters=downparameters, up_parameters=upparameters, augment=False).to('cuda')\n# net = smp.Unet('tu-maxxvit_rmlp_small_rw_256.sw_in1k',classes=1, encoder_depth=4, decoder_channels=[512,256,128,64]).to('cuda')\n# net = smp.Unet('tu-coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k',classes=1, encoder_depth=4, decoder_channels=[512,256,128,64]).to('cuda') \nnet.load_state_dict(state_dict)\nnet.eval()\n\n_, PA, IoU, precision, recall, F1, dice = validate(net, pad=pad, device='cuda', usage='cross-validate', test_batch_size=3)\n\nprint(f'Pixel accuracy: {PA}')\nprint(f'Jaccard Index: {IoU}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\nprint(f'F1 Score: {F1}')\nprint(f'Dice coefficient: {dice}', end='\\n')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:42:40.036248Z","iopub.execute_input":"2024-05-06T08:42:40.036504Z","iopub.status.idle":"2024-05-06T08:45:22.117641Z","shell.execute_reply.started":"2024-05-06T08:42:40.036482Z","shell.execute_reply":"2024-05-06T08:45:22.116719Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Pixel accuracy: 0.0010508411335495283\nJaccard Index: 0.28538557769636314\nPrecision: 0.3599159304002885\nRecall: 0.5795069994465765\nF1 Score: 0.444046646622291\nDice coefficient: 0.444046646622291\n","output_type":"stream"}]}]}
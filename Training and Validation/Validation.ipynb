{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6ffbef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T16:01:15.709195Z",
     "iopub.status.busy": "2024-05-14T16:01:15.708841Z",
     "iopub.status.idle": "2024-05-14T16:01:22.381803Z",
     "shell.execute_reply": "2024-05-14T16:01:22.380950Z"
    },
    "papermill": {
     "duration": 6.6813,
     "end_time": "2024-05-14T16:01:22.384212",
     "exception": false,
     "start_time": "2024-05-14T16:01:15.702912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as T\n",
    "import numpy as np\n",
    "\n",
    "# Generalized UNet architecture from scratch for learning purposes. See https://arxiv.org/abs/1505.04597\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "\n",
    "     # In the UNet architecture, this is the smallest possible block.\n",
    "     # Each of these applies a 2D convolution, normalizes the output and passes it through a ReLU activation function.\n",
    "     # The result of passing an input of size [N, in, H_in, W_in] through this layer is an output [N, out, H_out, W_out]\n",
    "     #    where H_out = (H_in + 2*padding - dilation*(kernel_size-1) - 1)/stride + 1 \n",
    "     #          W_out = (W_in + 2*padding - dilation*(kernel_size-1) - 1)/stride + 1 \n",
    "     # This output corresponds to the element_wise result of the ReLU activation function of the normalized convolution output.\n",
    "\n",
    "     # The block is used several times at each \"level\", which is why we later define a CNNSet, which is composed of several \n",
    "     # usages of the CNNBlock sequentially \n",
    "\n",
    "     \"\"\"\n",
    "     Parameters:\n",
    "     in_channels (int): Number of channels in the input to the block.\n",
    "     out_channels (int): Number of channels produced by the convolution of the input within the block.\n",
    "     kernel_size (int): Size of the convolving kernel. Default = 3.\n",
    "     stride (int) : Stride of the convolution. Default = 1.\n",
    "     padding (int) : Padding added to all four sides of the input. Default = 0.\n",
    "     \"\"\"\n",
    "\n",
    "     def __init__(self, in_channels: int, out_channels:int, kernel_size:int=3, stride:int=1, padding:int=0):\n",
    "          super(CNNBlock, self).__init__()\n",
    "\n",
    "          self.sequential_block = nn.Sequential(\n",
    "               nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "               nn.BatchNorm2d(out_channels),\n",
    "               nn.ReLU(inplace=True)\n",
    "          )\n",
    "\n",
    "     def forward(self, x):\n",
    "          x = self.sequential_block(x)\n",
    "          return x\n",
    "     \n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "     # The encoder part of the UNet consists of the downsampling of the initial input through the application of \n",
    "     # a sequential series of convolution sets followed by a max pooling layer. The complete operation's objective \n",
    "     # is to capture the context and spatial information of the input image at different scales.\n",
    "\n",
    "     \"\"\"\n",
    "     Parameters:\n",
    "     in_channels (int): Number of input channels of the first CNNSet.\n",
    "     out_channels (int): Number of output channels of the first CNNSet.\n",
    "     padding (int): Padding applied in each convolution.\n",
    "     levels (int): Number times a CNNSet + MaxPool2D layer is applied.\n",
    "     \"\"\"\n",
    "\n",
    "     def __init__(self, input_channels:int, output_channels:int, pool_kernelsize: int, parameters: list):\n",
    "          super(Encoder, self).__init__()\n",
    "          self.encoder_layers = nn.ModuleList()\n",
    "          levels = len(parameters)\n",
    "          for level in range(levels-1):\n",
    "               for conv in range(len(parameters[level])):\n",
    "                    conv_kernelsize = parameters[level][conv][0]\n",
    "                    conv_stride = parameters[level][conv][1]\n",
    "                    self.encoder_layers.append(CNNBlock(in_channels=input_channels, out_channels=output_channels, kernel_size=conv_kernelsize, stride=conv_stride))\n",
    "                    input_channels = output_channels\n",
    "               output_channels *= 2\n",
    "               self.encoder_layers.append(nn.MaxPool2d(pool_kernelsize))\n",
    "          # A final convolution set is applied after all the levels, commonly referred to as the bottleneck\n",
    "          for conv in range(len(parameters[-1])):\n",
    "               conv_kernelsize = parameters[-1][conv][0]\n",
    "               conv_stride = parameters[-1][conv][1]\n",
    "               self.encoder_layers.append(CNNBlock(in_channels=input_channels, out_channels=output_channels, kernel_size=conv_kernelsize, stride=conv_stride))\n",
    "               input_channels = output_channels\n",
    "\n",
    "     def forward(self,x):\n",
    "          residual_connection = []\n",
    "          for i, layer in enumerate(self.encoder_layers):\n",
    "               x = layer(x)\n",
    "               # After the set CNN is processed, the result is logged to be sent in a connection to the decoder\n",
    "               if i<len(self.encoder_layers)-1 and isinstance(self.encoder_layers[i+1], nn.MaxPool2d):\n",
    "                    residual_connection.append(x)\n",
    "               # If the processed layer is a pooling operation, the result is not logged\n",
    "          return x, residual_connection\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "     \"\"\"\n",
    "     Parameters:\n",
    "     in_channels (int): Number of input channels of the first up-convolution layer.\n",
    "     out_channels (int): Number of output channels of the first up-convolution layer.\n",
    "     padding (int): Padding applied in each convolution.\n",
    "     levels (int): number times an up-convolution + CNNSet is applied.\n",
    "     \"\"\"\n",
    "\n",
    "     # After the encoder has downsampled the information, the decoder now applies an upsampling to match to original features.\n",
    "     # This is achieved combining up-convolutions followed by convolution sets sequentially, achieving a recovery of the \n",
    "     # fine-grained spatial information lost during the downsampling in the encoder.\n",
    "\n",
    "     def __init__(self, input_channels:int, exit_channels:int, uppool_kernelsize:int, parameters: list):\n",
    "          super(Decoder, self).__init__()\n",
    "          self.exit_channels = exit_channels\n",
    "          self.decoder_layers = nn.ModuleList()\n",
    "\n",
    "          levels = len(parameters)\n",
    "          for level in range(levels-1):\n",
    "               output_channels = int(input_channels/2)\n",
    "               self.decoder_layers.append(nn.ConvTranspose2d(in_channels=input_channels, out_channels=output_channels, kernel_size=uppool_kernelsize, stride=uppool_kernelsize))\n",
    "               for conv in range(len(parameters[level])):\n",
    "                    conv_kernelsize = parameters[level][conv][0]\n",
    "                    conv_stride = parameters[level][conv][1]\n",
    "                    self.decoder_layers.append(CNNBlock(in_channels=input_channels, out_channels=output_channels, kernel_size=conv_kernelsize, stride=conv_stride))\n",
    "                    input_channels = output_channels\n",
    "          # A final convolution set without the ReLU activation function since the output will be passed through a BCELoss \n",
    "          self.decoder_layers.append(nn.Conv2d(in_channels=input_channels, out_channels=exit_channels, kernel_size=1))\n",
    "\n",
    "     def forward(self, x, residual_connection):\n",
    "          for i, layer in enumerate(self.decoder_layers):\n",
    "               # After the previous output is up-sampled, the connection from the equivalent level is concatenated\n",
    "               if i>0 and isinstance(self.decoder_layers[i-1], nn.ConvTranspose2d):\n",
    "                    # First we center-crop the route tensor to make the size match\n",
    "                    residual_connection[-1] = T.center_crop(residual_connection[-1], x.shape[2])\n",
    "                    # Then we concatenate the tensors in the dimensions of the channels\n",
    "                    x = torch.cat([x, residual_connection.pop(-1)], dim=1)\n",
    "                    x = layer(x)\n",
    "               # If the processed layer is an up-convolution operation, the connection is not performed\n",
    "               else:\n",
    "                    x = layer(x)\n",
    "          return x\n",
    "\n",
    "\n",
    "class UNetV2(nn.Module):\n",
    "\n",
    "     \"\"\"\n",
    "     Parameters:\n",
    "     in_channels (int): Number of input channels.\n",
    "     first_out_channels (int): Number of output channels of the first convolution set.\n",
    "     exit_channels (int): Number of output channels.\n",
    "     levels (int): Number of levels for the encoder-decoder architecture.\n",
    "     padding (int): Padding applied in each convolution operation.\n",
    "     \"\"\"\n",
    "\n",
    "     # After the encoder has downsampled the information, the decoder now applies an upsampling to match to original features.\n",
    "     # This is achieved combining up-convolutions followed by convolution sets sequentially, achieving a recovery of the \n",
    "     # fine-grained spatial information lost during the downsampling in the encoder.\n",
    "\n",
    "     def __init__(self, in_channels, first_out_channels, exit_channels, pool_kernelsize, down_parameters, up_parameters, augment=False):\n",
    "          super(UNetV2, self).__init__()\n",
    "          levels = len(down_parameters)\n",
    "          self.encoder = Encoder(input_channels=in_channels, output_channels=first_out_channels, pool_kernelsize=pool_kernelsize, parameters=down_parameters)\n",
    "          self.decoder = Decoder(input_channels=first_out_channels*(2**(levels-1)), exit_channels=exit_channels, uppool_kernelsize=pool_kernelsize, parameters=up_parameters)\n",
    "          self.augment = augment\n",
    "        \n",
    "     def forward(self, x):\n",
    "          encoder_out, residuals = self.encoder(x)\n",
    "          decoder_out = self.decoder(encoder_out, residuals)\n",
    "          if self.augment: \n",
    "              return T.center_crop(decoder_out, (512,512))\n",
    "          else:\n",
    "              return T.center_crop(decoder_out, (256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "013d9fdc",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-14T16:01:22.393477Z",
     "iopub.status.busy": "2024-05-14T16:01:22.393069Z",
     "iopub.status.idle": "2024-05-14T16:01:24.041356Z",
     "shell.execute_reply": "2024-05-14T16:01:24.040460Z"
    },
    "papermill": {
     "duration": 1.655462,
     "end_time": "2024-05-14T16:01:24.043783",
     "exception": false,
     "start_time": "2024-05-14T16:01:22.388321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import albumentations as A\n",
    "\n",
    "def create_grid(nc: int, offset=0.5) -> torch.Tensor:\n",
    "    grid = np.zeros((nc, nc, 2), dtype=np.float32)\n",
    "    for ix in range(nc):\n",
    "        for iy in range(nc):\n",
    "            grid[ix, iy, 1] = -1 + 2 * (ix + 0.5) / nc + offset / 128\n",
    "            grid[ix, iy, 0] = -1 + 2 * (iy + 0.5) / nc + offset / 128\n",
    "    grid = torch.from_numpy(grid).unsqueeze(0)\n",
    "    return grid\n",
    "\n",
    "def img2tensor(img, dtype: np.dtype = np.float32):\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    tensor = torch.from_numpy(img.astype(dtype, copy=False))\n",
    "    return tensor\n",
    "\n",
    "class ContrailsDataset(Dataset):\n",
    "\n",
    "     def __init__(self, path, use='train', soft_labels=False, only_positives=True, repeat=1, augment=False):\n",
    "          if use == 'train' or use=='metrics':\n",
    "               train = True\n",
    "          else:\n",
    "               train = False\n",
    "          self.path = os.path.join(path, \"train\" if train else \"validation\", \"images\")\n",
    "          if only_positives:\n",
    "               positives_path = '/kaggle/input/positives'\n",
    "               positives_file = np.load(os.path.join(positives_path,\"positive_train.npy\" if train else \"positive_validation.npy\"))\n",
    "               positives_fnames = [filename.split(\"\\\\\")[3] for filename in positives_file]\n",
    "               if use == 'train' or use == 'cross-validate':\n",
    "                    self.filenames = [filename.split(\".\")[0] for filename in os.listdir(self.path) if filename.split(\".\")[0] in positives_fnames]\n",
    "               elif use == 'metrics':\n",
    "                    self.filenames = random.sample([filename.split(\".\")[0] for filename in os.listdir(self.path) if filename.split(\".\")[0] in positives_fnames], 500)\n",
    "          else:\n",
    "               if use == 'train' or use == 'cross-validate':\n",
    "                    self.filenames = [filename.split(\".\")[0] for filename in os.listdir(self.path)]\n",
    "               elif use == 'metrics':\n",
    "                    self.filenames = random.sample([filename.split(\".\")[0] for filename in os.listdir(self.path)],500)\n",
    "          self.train = train\n",
    "          self.nc = 3\n",
    "          self.repeat = repeat\n",
    "          self.soft_labels = soft_labels\n",
    "          self.augment = augment \n",
    "          if self.augment:\n",
    "               self.grid = create_grid(512, offset=0.5)\n",
    "\n",
    "     def __len__(self):\n",
    "          return self.repeat * len(self.filenames)\n",
    "     \n",
    "     def __getitem__(self, index):\n",
    "          index = index % len(self.filenames)\n",
    "          filename = self.filenames[index]\n",
    "          try:\n",
    "               image = np.array(Image.open(os.path.join(self.path, self.filenames[index] + '.png')))\n",
    "               if self.soft_labels:\n",
    "                    mask  = np.load(os.path.join(self.path.replace('images','soft_label'), self.filenames[index] + '.npy'))\n",
    "               else:\n",
    "                    mask  = np.load(os.path.join(self.path.replace('images','ground_truth'), self.filenames[index] + '.npy'))\n",
    "               image_tensor, mask_tensor = img2tensor(image/255), img2tensor(mask)   # Sizes 3x256x256 and 1x256x256  \n",
    "               if self.augment:\n",
    "                    transform = A.Compose([A.RandomRotate90(p=0.05),A.HorizontalFlip(p=0.05)])\n",
    "                    image_tensor = T.resize(image_tensor,512)\n",
    "                    mask_sym = F.grid_sample(mask_tensor.unsqueeze(0), self.grid, mode='bilinear', padding_mode='border', \n",
    "                                             align_corners=False).squeeze(0)\n",
    "                    image_np = image_tensor.permute(1, 2, 0).numpy()\n",
    "                    mask_np = mask_sym.permute(1, 2, 0).numpy()\n",
    "                    # Apply augmentation\n",
    "                    aug = transform(image=image_np, mask=mask_np)\n",
    "                    transformed_image = aug['image'].transpose(2, 0, 1)\n",
    "                    transformed_mask = aug['mask'].transpose(2, 0, 1)\n",
    "                    image_tensor = torch.from_numpy(transformed_image)\n",
    "                    mask_tensor = torch.from_numpy(transformed_mask)\n",
    "               return image_tensor, mask_tensor, filename\n",
    "          except Exception as e:\n",
    "               print(f\"\\n Error loading file: {e} \\n\")\n",
    "               return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "844b79a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T16:01:24.053066Z",
     "iopub.status.busy": "2024-05-14T16:01:24.052488Z",
     "iopub.status.idle": "2024-05-14T16:01:24.070606Z",
     "shell.execute_reply": "2024-05-14T16:01:24.069636Z"
    },
    "papermill": {
     "duration": 0.025067,
     "end_time": "2024-05-14T16:01:24.072794",
     "exception": false,
     "start_time": "2024-05-14T16:01:24.047727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def validate(net, usage, device, pad, positives=False, test_batch_size=50, threshold=0.5):    \n",
    "\n",
    "     testset = ContrailsDataset(path='/kaggle/input/opencontrails-png/SingleFrame_PNG', use=usage, augment=False, only_positives=positives)\n",
    "     testloader = torch.utils.data.DataLoader(testset,batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "     criterion = nn.BCEWithLogitsLoss()\n",
    "     \n",
    "     pixels = 256*256\n",
    "\n",
    "     # Positive pixels labelled as positive\n",
    "     TP = 0\n",
    "     # Negative pixels labelled as negative\n",
    "     TN = 0\n",
    "     # Positive pixels labelled as negative\n",
    "     FN = 0\n",
    "     # Negative pixels labelled as positive\n",
    "     FP = 0\n",
    "     # Loss for each image\n",
    "     individual_loss = []\n",
    "     # Percentages of TP, TN, FN and FP for each image\n",
    "     confusion_matrix = []\n",
    "     # List of processed images\n",
    "     fnames = []\n",
    "\n",
    "     running_loss = 0\n",
    "\n",
    "     with torch.no_grad():\n",
    "          for i, data in enumerate(testloader):\n",
    "               images, labels, filename = data\n",
    "               images, labels = images.to(device), labels.to(device)\n",
    "               images = F.pad(images,(pad,pad,pad,pad), mode='reflect')\n",
    "               outputs = torch.sigmoid(net(images))\n",
    "               outputs = T.center_crop(outputs, (256,256))\n",
    "               outputs = outputs.view(-1,1,256,256)\n",
    "               binary_outputs = (outputs > threshold).float()\n",
    "\n",
    "               loss = criterion(outputs, labels)\n",
    "            \n",
    "               individual_loss.append(loss.item())\n",
    "               running_loss += loss.item()\n",
    "\n",
    "               TP_ind = torch.sum((binary_outputs == 1) & (labels == 1)).item()\n",
    "               TN_ind = torch.sum((binary_outputs == 0) & (labels == 0)).item()\n",
    "               FN_ind = torch.sum((binary_outputs == 0) & (labels == 1)).item()\n",
    "               FP_ind = torch.sum((binary_outputs == 1) & (labels == 0)).item()\n",
    "            \n",
    "               TP += TP_ind\n",
    "               TN += TN_ind\n",
    "               FN += FN_ind\n",
    "               FP += FP_ind\n",
    "            \n",
    "               confusion_matrix.append([TP_ind/pixels,TN_ind/pixels,FN_ind/pixels,FP_ind/pixels])\n",
    "               fnames.append(filename)\n",
    "               print(f'Processing batch {i+1}/{len(testloader)}', end='\\r')\n",
    "\n",
    "     # Pixel Accuracy\n",
    "     PA = TP/(TP+TN+FP+FN) if (TP+TN+FP+FN)>0 else '-'\n",
    "     # Jaccard Coefficient\n",
    "     IoU = TP/(TP+FP+FN) if (TP+FP+FN)>0 else '-'\n",
    "     # Precision\n",
    "     precision = TP/(TP+FP) if (TP+FP)>0 else '-'\n",
    "     # Recall\n",
    "     recall = TP/(TP+FN) if (TP+FN)>0 else '-'\n",
    "     # F1 Score\n",
    "     F1 = 2*(precision*recall/(precision+recall)) if ((TP+FP)>0 and (TP+FN)>0 and TP>0) else '-'\n",
    "     # Dice Coefficient\n",
    "     dice = 2*TP/(2*TP+FP+FN) if (TP+FP+FN)>0 else '-'\n",
    "\n",
    "     return running_loss/len(testset), PA, IoU, precision, recall, F1, dice, individual_loss, confusion_matrix, fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee0d171b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T16:01:24.082235Z",
     "iopub.status.busy": "2024-05-14T16:01:24.081347Z",
     "iopub.status.idle": "2024-05-14T16:01:48.268632Z",
     "shell.execute_reply": "2024-05-14T16:01:48.267545Z"
    },
    "papermill": {
     "duration": 24.19442,
     "end_time": "2024-05-14T16:01:48.271153",
     "exception": false,
     "start_time": "2024-05-14T16:01:24.076733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation-models-pytorch\r\n",
      "  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl.metadata (30 kB)\r\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.16.2)\r\n",
      "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\r\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hCollecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\r\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting timm==0.9.2 (from segmentation-models-pytorch)\r\n",
      "  Downloading timm-0.9.2-py3-none-any.whl.metadata (68 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (4.66.1)\r\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (9.5.0)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.2)\r\n",
      "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\r\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.22.2)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.4)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.31.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2024.2.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (21.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2024.2.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.1.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\r\n",
      "Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading timm-0.9.2-py3-none-any.whl (2.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\r\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=612370fe9a32ed08c45aabb75a6bc293c13ca7ff3eb59754d8efbb3717574353\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\r\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=45eab860b0cf2fdd25945e4bdb0a09dd84161cb180d30c55c3bcf05fcd95877f\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\r\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\r\n",
      "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\r\n",
      "  Attempting uninstall: timm\r\n",
      "    Found existing installation: timm 0.9.16\r\n",
      "    Uninstalling timm-0.9.16:\r\n",
      "      Successfully uninstalled timm-0.9.16\r\n",
      "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.3 timm-0.9.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation-models-pytorch\n",
    "\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df85dd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T16:01:48.286086Z",
     "iopub.status.busy": "2024-05-14T16:01:48.285765Z",
     "iopub.status.idle": "2024-05-14T16:01:52.754875Z",
     "shell.execute_reply": "2024-05-14T16:01:52.753997Z"
    },
    "papermill": {
     "duration": 4.479356,
     "end_time": "2024-05-14T16:01:52.757465",
     "exception": false,
     "start_time": "2024-05-14T16:01:48.278109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parameter_generator(levels, convs, kernel_size):\n",
    "     parameters = [[None for _ in range(convs)] for _ in range(levels)]\n",
    "     for level in range(levels):\n",
    "          for conv in range(convs):\n",
    "               if isinstance(kernel_size,list):\n",
    "                    parameters[level][conv] = [(kernel_size[level], kernel_size[level]), 1]\n",
    "               else:\n",
    "                    parameters[level][conv] = [(kernel_size, kernel_size), 1]\n",
    "     return parameters\n",
    "\n",
    "\n",
    "levs = 5\n",
    "cons = 2\n",
    "kers = 3\n",
    "path2trained = '/kaggle/input/trained-networks/Networks/MaxViT256_523_Positives_20epoch_T2.pth'\n",
    "pad = 0\n",
    "\n",
    "state_dict = torch.load(path2trained)\n",
    "downparameters = parameter_generator(levs,cons,kers)\n",
    "upparameters = parameter_generator(levs,cons,kers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3feeccfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T16:01:52.773467Z",
     "iopub.status.busy": "2024-05-14T16:01:52.773144Z",
     "iopub.status.idle": "2024-05-14T16:01:56.727856Z",
     "shell.execute_reply": "2024-05-14T16:01:56.726879Z"
    },
    "papermill": {
     "duration": 3.967652,
     "end_time": "2024-05-14T16:01:56.732563",
     "exception": false,
     "start_time": "2024-05-14T16:01:52.764911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e57a986cad4759bf5c3563b61da701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/264M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (encoder): TimmUniversalEncoder(\n",
       "    (model): FeatureListNet(\n",
       "      (stem): Stem(\n",
       "        (conv1): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (norm1): LayerNormAct2d(\n",
       "          (48,), eps=1e-06, elementwise_affine=True\n",
       "          (drop): Identity()\n",
       "          (act): GELU()\n",
       "        )\n",
       "        (conv2): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (stages_0): MaxxVitStage(\n",
       "        (blocks): Sequential(\n",
       "          (0): MaxxVitBlock(\n",
       "            (conv): ConvNeXtBlock(\n",
       "              (shortcut): Downsample2d(\n",
       "                (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "                (expand): Identity()\n",
       "              )\n",
       "              (down): Identity()\n",
       "              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=96)\n",
       "              (norm): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): ConvMlp(\n",
       "                (fc1): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (norm): Identity()\n",
       "                (act): GELU()\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (ls): LayerScale2d()\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): MaxxVitBlock(\n",
       "            (conv): ConvNeXtBlock(\n",
       "              (shortcut): Identity()\n",
       "              (down): Identity()\n",
       "              (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "              (norm): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): ConvMlp(\n",
       "                (fc1): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (norm): Identity()\n",
       "                (act): GELU()\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (ls): LayerScale2d()\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (stages_1): MaxxVitStage(\n",
       "        (blocks): Sequential(\n",
       "          (0): MaxxVitBlock(\n",
       "            (conv): ConvNeXtBlock(\n",
       "              (shortcut): Downsample2d(\n",
       "                (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "                (expand): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (down): Identity()\n",
       "              (conv_dw): Conv2d(96, 192, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=96)\n",
       "              (norm): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): ConvMlp(\n",
       "                (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (norm): Identity()\n",
       "                (act): GELU()\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (ls): LayerScale2d()\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=6, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=6, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): MaxxVitBlock(\n",
       "            (conv): ConvNeXtBlock(\n",
       "              (shortcut): Identity()\n",
       "              (down): Identity()\n",
       "              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "              (norm): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): ConvMlp(\n",
       "                (fc1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (norm): Identity()\n",
       "                (act): GELU()\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (ls): LayerScale2d()\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=6, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=6, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (stages_2): MaxxVitStage(\n",
       "        (blocks): Sequential(\n",
       "          (0): MaxxVitBlock(\n",
       "            (conv): ConvNeXtBlock(\n",
       "              (shortcut): Downsample2d(\n",
       "                (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "                (expand): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (down): Identity()\n",
       "              (conv_dw): Conv2d(192, 384, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=192)\n",
       "              (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): ConvMlp(\n",
       "                (fc1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (norm): Identity()\n",
       "                (act): GELU()\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (ls): LayerScale2d()\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=12, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=12, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): MaxxVitBlock(\n",
       "            (conv): ConvNeXtBlock(\n",
       "              (shortcut): Identity()\n",
       "              (down): Identity()\n",
       "              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "              (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): ConvMlp(\n",
       "                (fc1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (norm): Identity()\n",
       "                (act): GELU()\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (ls): LayerScale2d()\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=12, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=12, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): MaxxVitBlock(\n",
       "            (conv): ConvNeXtBlock(\n",
       "              (shortcut): Identity()\n",
       "              (down): Identity()\n",
       "              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "              (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): ConvMlp(\n",
       "                (fc1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (norm): Identity()\n",
       "                (act): GELU()\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (ls): LayerScale2d()\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=12, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=12, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): MaxxVitBlock(\n",
       "            (conv): ConvNeXtBlock(\n",
       "              (shortcut): Identity()\n",
       "              (down): Identity()\n",
       "              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "              (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): ConvMlp(\n",
       "                (fc1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (norm): Identity()\n",
       "                (act): GELU()\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (ls): LayerScale2d()\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=12, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=12, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): MaxxVitBlock(\n",
       "            (conv): ConvNeXtBlock(\n",
       "              (shortcut): Identity()\n",
       "              (down): Identity()\n",
       "              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "              (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): ConvMlp(\n",
       "                (fc1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (norm): Identity()\n",
       "                (act): GELU()\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (ls): LayerScale2d()\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=12, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=12, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (stages_3): MaxxVitStage(\n",
       "        (blocks): Sequential(\n",
       "          (0): MaxxVitBlock(\n",
       "            (conv): ConvNeXtBlock(\n",
       "              (shortcut): Downsample2d(\n",
       "                (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "                (expand): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (down): Identity()\n",
       "              (conv_dw): Conv2d(384, 768, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=384)\n",
       "              (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): ConvMlp(\n",
       "                (fc1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (norm): Identity()\n",
       "                (act): GELU()\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (ls): LayerScale2d()\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=24, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=24, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): MaxxVitBlock(\n",
       "            (conv): ConvNeXtBlock(\n",
       "              (shortcut): Identity()\n",
       "              (down): Identity()\n",
       "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "              (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): ConvMlp(\n",
       "                (fc1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (norm): Identity()\n",
       "                (act): GELU()\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "                (fc2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (ls): LayerScale2d()\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (attn_block): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=24, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (attn_grid): PartitionAttentionCl(\n",
       "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): AttentionCl(\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                (rel_pos): RelPosMlp(\n",
       "                  (bias_act): Identity()\n",
       "                  (mlp): Mlp(\n",
       "                    (fc1): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (act): ReLU()\n",
       "                    (drop1): Dropout(p=0.125, inplace=False)\n",
       "                    (norm): Identity()\n",
       "                    (fc2): Linear(in_features=512, out_features=24, bias=True)\n",
       "                    (drop2): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(1152, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(1216, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(608, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(352, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# net = UNetV2(in_channels=3, first_out_channels=64, exit_channels=1, pool_kernelsize=2, down_parameters=downparameters, up_parameters=upparameters, augment=False).to('cuda')\n",
    "# net = smp.Unet('tu-maxxvit_rmlp_small_rw_256.sw_in1k',classes=1, encoder_depth=4, decoder_channels=[512,256,128,64]).to('cuda')\n",
    "# net = smp.Unet('tu-coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k',classes=1, encoder_depth=4, decoder_channels=[512,256,128,64]).to('cuda') \n",
    "net = smp.Unet('tu-maxxvit_rmlp_small_rw_256',classes=1, encoder_depth=5, decoder_channels=[1024,512,256,128,64]).to('cuda')\n",
    "net.load_state_dict(state_dict)\n",
    "net.eval()\n",
    "\n",
    "# avg_loss, PA, IoU, precision, recall, F1, dice, individual_loss, confusion_matrix, fnames = validate(net, positives=False, pad=pad, device='cuda', usage='cross-validate', test_batch_size=1, threshold=0.7)\n",
    "\n",
    "# print(f'Pixel accuracy: {PA}')\n",
    "# print(f'Jaccard Index: {IoU}')\n",
    "# print(f'Precision: {precision}')\n",
    "# print(f'Recall: {recall}')\n",
    "# print(f'F1 Score: {F1}')\n",
    "# print(f'Dice coefficient: {dice}', end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "036bca87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T16:01:56.752162Z",
     "iopub.status.busy": "2024-05-14T16:01:56.751273Z",
     "iopub.status.idle": "2024-05-14T16:01:56.756340Z",
     "shell.execute_reply": "2024-05-14T16:01:56.755358Z"
    },
    "papermill": {
     "duration": 0.017418,
     "end_time": "2024-05-14T16:01:56.758700",
     "exception": false,
     "start_time": "2024-05-14T16:01:56.741282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# avgLoss = np.array(avg_loss)\n",
    "# np.save('avgLoss.npy', avgLoss)\n",
    "\n",
    "# pa = np.array(PA)\n",
    "# np.save('PA.npy', pa)\n",
    "\n",
    "# iou = np.array(IoU)\n",
    "# np.save('IoU.npy', iou)\n",
    "\n",
    "# prec = np.array(precision)\n",
    "# np.save('precision.npy', prec)\n",
    "\n",
    "# rec = np.array(recall)\n",
    "# np.save('recall.npy', rec)\n",
    "\n",
    "# f1 = np.array(F1)\n",
    "# np.save('F1.npy', f1)\n",
    "\n",
    "# dic = np.array(dice)\n",
    "# np.save('dice.npy', dic)\n",
    "\n",
    "# ind_loss = np.array(individual_loss)\n",
    "# np.save('individual_loss.npy', ind_loss)\n",
    "\n",
    "# conf_mat = np.array(confusion_matrix)\n",
    "# np.save('confusion_matrix.npy', conf_mat)\n",
    "\n",
    "# fnam = np.array(fnames)\n",
    "# np.save('filenames.npy', fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f20ad8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-14T16:01:56.777006Z",
     "iopub.status.busy": "2024-05-14T16:01:56.776665Z",
     "iopub.status.idle": "2024-05-14T16:30:19.012108Z",
     "shell.execute_reply": "2024-05-14T16:30:19.011092Z"
    },
    "papermill": {
     "duration": 1702.247547,
     "end_time": "2024-05-14T16:30:19.014649",
     "exception": false,
     "start_time": "2024-05-14T16:01:56.767102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying out threshold: 0.4\n",
      "Dice=0.4866516171587136\n",
      "\n",
      "Trying out threshold: 0.4210526315789474\n",
      "Dice=0.49878724058780377\n",
      "\n",
      "Trying out threshold: 0.4421052631578948\n",
      "Dice=0.5096911814165204\n",
      "\n",
      "Trying out threshold: 0.4631578947368421\n",
      "Dice=0.5200298803670379\n",
      "\n",
      "Trying out threshold: 0.4842105263157895\n",
      "Dice=0.5298830019598656\n",
      "\n",
      "Trying out threshold: 0.5052631578947369\n",
      "Dice=0.5387353094301296\n",
      "\n",
      "Trying out threshold: 0.5263157894736843\n",
      "Dice=0.5467029768980856\n",
      "\n",
      "Trying out threshold: 0.5473684210526316\n",
      "Dice=0.5542903352712332\n",
      "\n",
      "Trying out threshold: 0.5684210526315789\n",
      "Dice=0.5608936951565827\n",
      "\n",
      "Trying out threshold: 0.5894736842105264\n",
      "Dice=0.566456113156365\n",
      "\n",
      "Trying out threshold: 0.6105263157894737\n",
      "Dice=0.5715877370583795\n",
      "\n",
      "Trying out threshold: 0.631578947368421\n",
      "Dice=0.5760263537162857\n",
      "\n",
      "Trying out threshold: 0.6526315789473685\n",
      "Dice=0.5790505365075459\n",
      "\n",
      "Trying out threshold: 0.6736842105263159\n",
      "Dice=0.5818379054605262\n",
      "\n",
      "Trying out threshold: 0.6947368421052631\n",
      "Dice=0.5832585773983131\n",
      "\n",
      "Trying out threshold: 0.7157894736842105\n",
      "Dice=0.5832330320421962\n",
      "\n",
      "Trying out threshold: 0.736842105263158\n",
      "Dice=0.5814146353155485\n",
      "\n",
      "Trying out threshold: 0.7578947368421053\n",
      "Dice=0.578621678441161\n",
      "\n",
      "Trying out threshold: 0.7789473684210526\n",
      "Dice=0.5731163962635202\n",
      "\n",
      "Trying out threshold: 0.8\n",
      "Dice=0.5646388589039388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.linspace(0.4, 0.8, 20)\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "dices = []\n",
    "\n",
    "for th in thresholds:\n",
    "    print(f'Trying out threshold: {th}')\n",
    "    avg_loss, PA, IoU, precision, recall, F1, dice, individual_loss, confusion_matrix, fnames = validate(net, positives=False, pad=pad, device='cuda', usage='cross-validate', test_batch_size=1, threshold=th)\n",
    "    print(f'Dice={dice}\\n')\n",
    "    dices.append(dice)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall) \n",
    "    \n",
    "d = np.array(dices)\n",
    "np.save('dices_th.npy', d)\n",
    "\n",
    "p = np.array(precisions)\n",
    "np.save('precisions_th.npy', p)\n",
    "\n",
    "r = np.array(recalls)\n",
    "np.save('recalls_th.npy', r)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4717401,
     "sourceId": 8009157,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4765964,
     "sourceId": 8075890,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4951582,
     "sourceId": 8398625,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1749.391437,
   "end_time": "2024-05-14T16:30:22.120407",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-14T16:01:12.728970",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0b536e57c13440a484f88cc5ddb81e3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_75cfa68184f547b4a28fe46a5bfc4c0b",
       "max": 264098550.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ca8a97fc96e144e59ee2e0722d2b7805",
       "value": 264098550.0
      }
     },
     "0c808b1ad7dd492299a4887946a6bc5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1b5c553536e844b8be3c79692de789c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ed3291f9f6ca4156b8c43637d7e410e0",
       "placeholder": "​",
       "style": "IPY_MODEL_e10b51a7a4d5485e9907f315a65cfeb9",
       "value": "model.safetensors: 100%"
      }
     },
     "75cfa68184f547b4a28fe46a5bfc4c0b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "95e57a986cad4759bf5c3563b61da701": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1b5c553536e844b8be3c79692de789c8",
        "IPY_MODEL_0b536e57c13440a484f88cc5ddb81e3a",
        "IPY_MODEL_e9c481fde6e7475386a3f9b540844810"
       ],
       "layout": "IPY_MODEL_be01cd35781b45edb5e97ce4248ed88c"
      }
     },
     "a61dc3a409b54259b71267ef946ea894": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be01cd35781b45edb5e97ce4248ed88c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca8a97fc96e144e59ee2e0722d2b7805": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e10b51a7a4d5485e9907f315a65cfeb9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e9c481fde6e7475386a3f9b540844810": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a61dc3a409b54259b71267ef946ea894",
       "placeholder": "​",
       "style": "IPY_MODEL_0c808b1ad7dd492299a4887946a6bc5d",
       "value": " 264M/264M [00:01&lt;00:00, 216MB/s]"
      }
     },
     "ed3291f9f6ca4156b8c43637d7e410e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
